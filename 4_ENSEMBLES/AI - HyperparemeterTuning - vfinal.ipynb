{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "#sns.set_context(\"poster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter Tuning</h1>\n",
    "\n",
    "In machine learning our objective is to select the algorithm that whose bias fits better the task at hand, therefore the data and the objective that we want to accomplish. In order to achieve this goal we compare the performance of different algorithms against the goal with a concrete set of data. This is what we've done in the previous notebooks.\n",
    "\n",
    "However, once the most appropriate algorithm has been selected we can still improve its performance selecting the most appropriate parameters. \n",
    "\n",
    "The parameters that govern the behavior of an algorithm are called hyperparameters because they are not parameters of the model itself, but parameters of the algorithm. \n",
    "\n",
    "There are two main approaches to hyperparameter optimization:\n",
    "<blockquote>\n",
    "    <ul>\n",
    "        <li><b>Grid Search Parameter Tuning.</b> In this approach you provide a grid filled with potential options for paramters and you instruct the system to try them all. Once the results are presented to you, then you can pick up the ones that fit better your goal or initiate another search narrowing the search space.</li>\n",
    "        <li><b>Random Search Parameter Tuning.</b> In this approach instead of you providing the options is the algorithm that randomly searches them.</li>\n",
    "    </ul>\n",
    "</blockquote>\n",
    "\n",
    "Besides these two options, now we have systems that automatically search for the best parameters or the combination algorithm parameter. Two of the best known ones are auto-sklearn for scikit-learn, DataRobot (very popular in Finance) or the existing versions for every cloud platform. \n",
    "\n",
    "Again, in order to be able to compare them with the previous one, we will use the same dataset, the Pima Indians.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pima_indians_cowboy_1889.jpg\">\n",
    "\n",
    "In this exercise we will use one of the traditional Machine Learning dataset, the Pima Indians diabetes dataset.\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "Content\n",
    "The datasets consists of several medical predictor variables and one target variable, <b>Outcome</b>. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "<blockquote>\n",
    "        <ul style=\"list-style-type:square;\">\n",
    "            <li>Pregnancies</li> \n",
    "            <li>Glucose</li>\n",
    "            <li>BloodPressure</li>\n",
    "            <li>SkinThickness</li>\n",
    "            <li>Insulin</li>\n",
    "            <li>BMI</li>\n",
    "            <li>DiabetesPedigreeFunction (scores de likelihood of diabetes based on family history)</li>\n",
    "            <li>Age</li>\n",
    "            <li>Outcome</li>\n",
    "        </ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>glucose</th>\n",
       "      <th>pressure</th>\n",
       "      <th>skin</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnancies  glucose  pressure  skin  insulin   bmi   pedi  age  outcome\n",
       "0            6      148        72    35        0  33.6  0.627   50        1\n",
       "1            1       85        66    29        0  26.6  0.351   31        0\n",
       "2            8      183        64     0        0  23.3  0.672   32        1\n",
       "3            1       89        66    23       94  28.1  0.167   21        0\n",
       "4            0      137        40    35      168  43.1  2.288   33        1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,  33.6  ,   0.627,  50.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,  26.6  ,   0.351,  31.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,  23.3  ,   0.672,  32.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,  26.2  ,   0.245,  30.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,  30.1  ,   0.349,  47.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,  30.4  ,   0.315,  23.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1     2     3      4     5      6     7\n",
       "0  6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0\n",
       "1  1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0\n",
       "2  8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0\n",
       "3  1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0\n",
       "4  0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Pima indians dataset and separate input and output components \n",
    "\n",
    "from numpy import set_printoptions\n",
    "set_printoptions(precision=3)\n",
    "\n",
    "filename = \"pima-indians-diabetes.data.csv\"\n",
    "names = [\"pregnancies\", \"glucose\", \"pressure\", \"skin\", \"insulin\", \"bmi\", \"pedi\", \"age\", \"outcome\"]\n",
    "p_indians = pd.read_csv(filename, names=names)\n",
    "p_indians.head()\n",
    "\n",
    "# First we separate into input and output components\n",
    "array = p_indians.values\n",
    "X = array[:,0:8]\n",
    "y = array[:,8]\n",
    "np.set_printoptions(suppress = True)\n",
    "X\n",
    "pd.DataFrame(X).head()\n",
    "\n",
    "# Create the DataFrames for plotting\n",
    "resall = pd.DataFrame()\n",
    "res_w1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Parameter Tuning\n",
    "\n",
    "Grid Search is an approach that will evaluate methodically a model for a combination of parameters specified in a grid. \n",
    "\n",
    "You can perform Grid Searcch with the <b>GridSearchCV</b> class.\n",
    "\n",
    "In this case we evaluate different values of the alpha parameter for a Ridge regression classifier in the diabetes dataset. In the Ridge regression the alpha parameter is the regularization strength. If alpha is 0 we are performing a normal OLS regression, the higher the value the stronger the regularization and therefore reduces the complexity of the model reducing overfitting.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=7, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RidgeClassifier(alpha=1.0, class_weight=None,\n",
       "                                       copy_X=True, fit_intercept=True,\n",
       "                                       max_iter=None, normalize=False,\n",
       "                                       random_state=None, solver='auto',\n",
       "                                       tol=0.001),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'alpha': array([1.   , 0.1  , 0.01 , 0.001, 0.   , 0.   ])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Best Score 0.77086 Alpha 1.000\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Parameter Tuning\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits = 10, random_state=seed, shuffle = True)\n",
    "\n",
    "alphas = np.array([1, 0.1, 0.01, 0.001, 0.0001, 0])\n",
    "param_grid = dict(alpha = alphas)\n",
    "\n",
    "model = RidgeClassifier()\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = kfold)\n",
    "grid.fit(X,y)\n",
    "\n",
    "# the default score is accuracy \n",
    "\n",
    "print(f'Grid Best Score {grid.best_score_:.5f} Alpha {grid.best_estimator_.alpha:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=7, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False, random_state=7,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_features': array([3, 4, 5, 6, 7, 8])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Best Score 76.56357 N. of features 4.000\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Parameter Tuning\n",
    "#    now with Random Forests\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle = True)\n",
    "\n",
    "num_trees = 100\n",
    "num_features = 3\n",
    "\n",
    "num_features = np.array([3, 4, 5, 6, 7, 8])\n",
    "param_grid = dict(max_features=num_features)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=num_trees, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold)\n",
    "grid.fit(X,y)\n",
    "\n",
    "# the default score is accuracy \n",
    "\n",
    "print(f'Grid Best Score {grid.best_score_*100:.5f} N. of features {grid.best_estimator_.max_features:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=7, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False, random_state=7,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_features': [3, 4, 5, 6, 7, 8],\n",
       "                         'n_estimators': [50, 100, 150, 200, 250, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Best Score 77.21975 N. of features  3.000             N. of tress 300\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Parameter Tuning\n",
    "#    now with Random Forests\n",
    "#    a more typical search where you look for the optimal number of trees and features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)\n",
    "\n",
    "param_grid = {\"max_features\":[3, 4, 5, 6, 7, 8], \"n_estimators\":[50, 100, 150, 200, 250, 300]}\n",
    "\n",
    "model = RandomForestClassifier(random_state = seed)\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = kfold)\n",
    "grid.fit(X,y)\n",
    "\n",
    "# the default score is accuracy \n",
    "\n",
    "print(f'Grid Best Score {grid.best_score_*100:.5f} N. of features  {grid.best_estimator_.max_features:.3f} \\\n",
    "            N. of tress {grid.best_estimator_.n_estimators:3d}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search Parameter\n",
    "\n",
    "With Random Search Paramenter instead of providing a set of elements where to search you let the algorihm do the search using a random distribution (i.e. uniform). \n",
    "\n",
    "You perform a Random Search Parameter using the <b>RandomizezSearchCV</b> class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=10, random_state=7, shuffle=True),\n",
       "                   error_score=nan,\n",
       "                   estimator=RidgeClassifier(alpha=1.0, class_weight=None,\n",
       "                                             copy_X=True, fit_intercept=True,\n",
       "                                             max_iter=None, normalize=False,\n",
       "                                             random_state=None, solver='auto',\n",
       "                                             tol=0.001),\n",
       "                   iid='deprecated', n_iter=100, n_jobs=None,\n",
       "                   param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001D1D35E6DC8>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=7, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Search Best Score 0.77086 Alpha 0.076\n"
     ]
    }
   ],
   "source": [
    "# Randomized Search Parameter Tuning\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)\n",
    "\n",
    "param_grid = {\"alpha\": uniform()}\n",
    "\n",
    "model = RidgeClassifier()\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions = param_grid, n_iter = 100, cv = kfold, random_state = seed)\n",
    "grid.fit(X,y)\n",
    "\n",
    "# the default score is accuracy \n",
    "\n",
    "print(f'Randomized Search Best Score {grid.best_score_:.5f} Alpha {grid.best_estimator_.alpha:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=10, random_state=7, shuffle=True),\n",
       "                   error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_wei...ction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_score=False,\n",
       "                                                    random_state=7, verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='deprecated', n_iter=100, n_jobs=None,\n",
       "                   param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001D1D3682888>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=7, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Best Score 77.21463 N. of features 2.000\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Parameter Tuning\n",
    "#    now with Random Forests\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "param_grid = {\"max_features\": randint(1,8)} \n",
    "\n",
    "model = RandomForestClassifier(n_estimators = num_trees, random_state = 7)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions = param_grid, n_iter = 100, cv = kfold, random_state = seed)\n",
    "grid.fit(X,y)\n",
    "\n",
    "# the default score is accuracy \n",
    "\n",
    "print(f'Randomized Best Score {grid.best_score_*100:.5f} N. of features {grid.best_estimator_.max_features:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=10, random_state=7, shuffle=True),\n",
       "                   error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_wei...\n",
       "                                                    warm_start=False),\n",
       "                   iid='deprecated', n_iter=100, n_jobs=None,\n",
       "                   param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001D1D36A8A48>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001D1D36A8BC8>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=7, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Best Score 77.99556 N. of features  2.000             N. of tress 271\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Parameter Tuning\n",
    "#    now with Random Forests\n",
    "#    a more typical search where you look for the optimal number of trees and features\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "seed = 7\n",
    "kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)\n",
    "\n",
    "param_grid = {\"max_features\": randint(1,8) , \"n_estimators\": randint(50,300)}\n",
    "\n",
    "model = RandomForestClassifier(random_state = seed)\n",
    "grid = RandomizedSearchCV(estimator = model, param_distributions = param_grid, n_iter = 100, cv = kfold, random_state = seed)\n",
    "grid.fit(X,y)\n",
    "\n",
    "# the default score is accuracy \n",
    "\n",
    "print(f'Grid Best Score {grid.best_score_*100:.5f} N. of features  {grid.best_estimator_.max_features:.3f} \\\n",
    "            N. of tress {grid.best_estimator_.n_estimators:3d}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Machine Learning Models\n",
    "\n",
    "After developing and training our model we need to save it for production. This implies saving the model and loading it when needed in order to do the predictions that we need (model.prdict(X)). \n",
    "\n",
    "Saving a data structure (a model is a data structure) for further use is called serializing. There are two main libraries that address this need:\n",
    "<blockquote>\n",
    "    <ul>\n",
    "        <li><b>Pickle.</b> The standard Python library for serialization.</li>\n",
    "        <li><b>Joblib.</b> The serialization library in the SciPy ecosystem.</li>\n",
    "    </ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle\n",
    "\n",
    "Pickle is the standard library for serialization in Python. You can use it to save your model to a file and later on load the file and use it to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model - Accuracy 76.190% \n"
     ]
    }
   ],
   "source": [
    "# Pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "seed = 7\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(solver = \"liblinear\")\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Now we save it into a file\n",
    "filename = \"log_model.sav\"\n",
    "dump(model, open(filename, \"wb\"))\n",
    "\n",
    "# .... some time later .... \n",
    "\n",
    "#load the model from disk\n",
    "loaded_model = load(open(filscoreename, \"rb\"))\n",
    "result = loaded_model.(X_test,y_test)\n",
    "\n",
    "print(f'Loaded model - Accuracy {result.mean()*100:.3f}% ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joblib\n",
    "\n",
    "Joblib is the library of the SciPy ecosystem that serializes numpy structures. \n",
    "\n",
    "It is highly optimized doing this job very efficiently, this is why is very interesting when models are large or they include the dataset (e.g. Knn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model - Accuracy 76.190% \n"
     ]
    }
   ],
   "source": [
    "# Joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "\n",
    "seed = 7\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(solver=\"liblinear\")\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Now we save it into a file\n",
    "filename = \"log_model-j.sav\"\n",
    "dump(model, open(filename, \"wb\"))\n",
    "\n",
    "\n",
    "# .... some time later .... \n",
    "\n",
    "\n",
    "#load the model from disk\n",
    "loaded_model = load(open(filename, \"rb\"))\n",
    "result = loaded_model.score(X_test,y_test)\n",
    "\n",
    "print(f'Loaded model - Accuracy {result.mean()*100:.3f}% ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"red\" size=6>Mission 1</font>\n",
    "\n",
    "a) Use the grid search paramenter for finding the best parameters for Random Forest with the Titanic dataset.<br><br>\n",
    "b) Same with the Random Search Parameter.<br><br>\n",
    "c) Serialize you Random Forest Titanic model.<br><br>\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Grid Search Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "titanic[\"Gender\"] = titanic[\"Sex\"].apply(lambda d: 1 if d == \"female\" else 0)\n",
    "\n",
    "titanic.drop([\"Name\", \"Sex\"], axis = 1, inplace = True)\n",
    "\n",
    "# Separate x and y\n",
    "\n",
    "array = titanic.values\n",
    "y = array[:,0]\n",
    "x = array[0:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gspt_ridge(x,y):\n",
    "\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits = 10, random_state=seed, shuffle = True)\n",
    "\n",
    "    alphas = np.array([1, 0.1, 0.01, 0.001, 0.0001, 0])\n",
    "    param_grid = dict(alpha = alphas)\n",
    "\n",
    "    model = RidgeClassifier()\n",
    "    grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = kfold)\n",
    "    grid.fit(x,y)\n",
    "\n",
    "    # the default score is accuracy \n",
    "\n",
    "    print(f'Grid Best Score {grid.best_score_:.5f} Alpha {grid.best_estimator_.alpha:.3f}')\n",
    "    \n",
    "    \n",
    "def gspt_rf(x,y): \n",
    "\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits=10, random_state=seed, shuffle = True)\n",
    "\n",
    "    num_trees = 100\n",
    "    num_features = 3\n",
    "\n",
    "    num_features = np.array([3, 4, 5, 6])\n",
    "    param_grid = dict(max_features=num_features)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=num_trees, random_state=seed)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold)\n",
    "    grid.fit(x,y)\n",
    "\n",
    "    # the default score is accuracy \n",
    "\n",
    "    print(f'Grid Best Score {grid.best_score_*100:.5f} N. of features {grid.best_estimator_.max_features:.3f}')\n",
    "    \n",
    "\n",
    "def gspt_rf_notree(x,y):\n",
    "                       \n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)\n",
    "\n",
    "    param_grid = {\"max_features\":[3, 4, 5, 6], \"n_estimators\":[50, 100, 150, 200, 250, 300]}\n",
    "\n",
    "    model = RandomForestClassifier(random_state = seed)\n",
    "    grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = kfold)\n",
    "    grid.fit(x,y)\n",
    "\n",
    "    # the default score is accuracy \n",
    "\n",
    "    print(f'Grid Best Score {grid.best_score_*100:.5f} N. of features  {grid.best_estimator_.max_features:.3f} \\\n",
    "                N. of trees {grid.best_estimator_.n_estimators:3d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Best Score 0.80151 Alpha 1.000\n",
      "Grid Best Score 82.40807 N. of features 4.000\n",
      "Grid Best Score 82.40807 N. of features  4.000                 N. of trees 100\n"
     ]
    }
   ],
   "source": [
    "gspt_ridge(x,y)\n",
    "gspt_rf(x,y)\n",
    "gspt_rf_notree(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Random Search Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs_ridge(x,y):\n",
    "\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)\n",
    "\n",
    "    param_grid = {\"alpha\": uniform()}\n",
    "\n",
    "    model = RidgeClassifier()\n",
    "    grid = RandomizedSearchCV(estimator=model, param_distributions = param_grid, n_iter = 100, cv = kfold, random_state = seed)\n",
    "    grid.fit(x,y)\n",
    "\n",
    "    # the default score is accuracy \n",
    "\n",
    "    print(f'Randomized Search Best Score {grid.best_score_:.5f} Alpha {grid.best_estimator_.alpha:.3f}')\n",
    "    \n",
    "\n",
    "def rs_rf(x,y):\n",
    "\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)\n",
    "\n",
    "    num_trees = 100\n",
    "\n",
    "    param_grid = {\"max_features\": randint(1,6)} \n",
    "\n",
    "    model = RandomForestClassifier(n_estimators = num_trees, random_state = 7)\n",
    "    grid = RandomizedSearchCV(estimator=model, param_distributions = param_grid, n_iter = 100, cv = kfold, random_state = seed)\n",
    "    grid.fit(x,y)\n",
    "\n",
    "    # the default score is accuracy \n",
    "\n",
    "    print(f'Randomized Best Score {grid.best_score_*100:.5f} N. of features {grid.best_estimator_.max_features:.3f}')\n",
    "    \n",
    "\n",
    "def rs_rf_notrees(x,y):\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)\n",
    "\n",
    "    param_grid = {\"max_features\": randint(1,6) , \"n_estimators\": randint(50,300)}\n",
    "\n",
    "    model = RandomForestClassifier(random_state = seed)\n",
    "    grid = RandomizedSearchCV(estimator = model, param_distributions = param_grid, n_iter = 100, cv = kfold, random_state = seed)\n",
    "    grid.fit(x,y)\n",
    "\n",
    "    # the default score is accuracy \n",
    "\n",
    "    print(f'Grid Best Score {grid.best_score_*100:.5f} N. of features  {grid.best_estimator_.max_features:.3f} \\\n",
    "                N. of tress {grid.best_estimator_.n_estimators:3d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Search Best Score 0.80151 Alpha 0.780\n",
      "Randomized Best Score 82.40807 N. of features 4.000\n",
      "Grid Best Score 82.52171 N. of features  4.000                 N. of tress 109\n"
     ]
    }
   ],
   "source": [
    "rs_ridge(x,y)\n",
    "rs_rf(x,y)\n",
    "rs_rf_notrees(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialization\n",
    "\n",
    "def ser_pickle(x,y):\n",
    "    \n",
    "    seed = 7\n",
    "    X_train,X_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=seed)\n",
    "\n",
    "    model = LogisticRegression(solver = \"liblinear\")\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    # Now we save it into a file\n",
    "    filename = \"log_model.sav\"\n",
    "    dump(model, open(filename, \"wb\"))\n",
    "\n",
    "    # .... some time later .... \n",
    "\n",
    "    #load the model from disk\n",
    "    loaded_model = load(open(filename, \"rb\"))\n",
    "    result = loaded_model.score(X_test,y_test)\n",
    "\n",
    "    print(f'Loaded model - Accuracy {result.mean()*100:.3f}% ')\n",
    "    \n",
    "    \n",
    "def ser_joblib(x,y):\n",
    "    \n",
    "    seed = 7\n",
    "    X_train,X_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=seed)\n",
    "\n",
    "    model = LogisticRegression(solver=\"liblinear\")\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    # Now we save it into a file\n",
    "    filename = \"log_model-j.sav\"\n",
    "    dump(model, open(filename, \"wb\"))\n",
    "\n",
    "    # .... some time later .... \n",
    "\n",
    "    #load the model from disk\n",
    "    loaded_model = load(open(filename, \"rb\"))\n",
    "    result = loaded_model.score(X_test,y_test)\n",
    "\n",
    "    print(f'Loaded model - Accuracy {result.mean()*100:.3f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model - Accuracy 80.524% \n",
      "Loaded model - Accuracy 80.524% \n"
     ]
    }
   ],
   "source": [
    "ser_pickle(x,y)\n",
    "ser_joblib(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
